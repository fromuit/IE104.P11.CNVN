{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup các kiểu\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import csv\n",
    "\n",
    "# Nhớ sửa mấy link đến folder và file \n",
    "folder_path = r\"D:\\IE104.P11.CNVN\\truyen\\1-seirei-tsukai-no-blade-dance\"   \n",
    "output_excel_path = r'hako_dataset.xlsx'\n",
    "output_csv_path = r'hako_dataset.csv'\n",
    "\n",
    "# Nhớ chỉnh sửa max_files ở dưới trước khi trích xuất và lưu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tổng số file html trong folder là: 272\n"
     ]
    }
   ],
   "source": [
    "# Các hàm xử lý phụ\n",
    "# Hàm xử lý ngày tháng năm\n",
    "def parse_date(date_text):\n",
    "    try:\n",
    "        date_obj = datetime.strptime(date_text, '%d/%m/%Y')\n",
    "        return date_obj.day, date_obj.month, date_obj.year\n",
    "    except ValueError:\n",
    "        print(f\"Unable to parse date: {date_text}\")\n",
    "        return None, None, None\n",
    "    \n",
    "# Hàm để xử lý giá trị rỗng\n",
    "def process_value(value, is_numeric=False):\n",
    "    if value in [None, 'nan', 'NaN', 'N/A', '']:\n",
    "        return 0 if is_numeric else \"NOT FOUND\"\n",
    "    return value    \n",
    "\n",
    "# Hàm chuyển đổi giá trị thành số\n",
    "def convert_to_number(value):\n",
    "    if isinstance(value, (int, float)):\n",
    "        return value\n",
    "    if isinstance(value, str):\n",
    "        value = ''.join(filter(str.isdigit, value))\n",
    "        return int(value) if value else 0\n",
    "    return 0\n",
    "\n",
    "# Hàm đếm số file html trong folder (có cải tiến lại vì trong folder có thể có folder con)\n",
    "def count_html_files(folder_path):\n",
    "    html_count = 0\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.html'):\n",
    "                html_count += 1\n",
    "    return html_count\n",
    "\n",
    "total = count_html_files(folder_path)   # Đếm số file html trong folder\n",
    "print(f\"Tổng số file html trong folder là: {total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Các hàm trích xuất thông tin (muốn bỏ thông tin gì thì đóng comment đoạn code trích xuất thông tin đó)\n",
    "\n",
    "# Trích xuất ID từ tên file\n",
    "def extract_id_from_filename(filename):\n",
    "    match = re.match(r'^(\\\\d+)', filename)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "# Trích xuất thông tin từ file HTML\n",
    "def extract_info_from_html(file_path):\n",
    "    try:\n",
    "        file_id = extract_id_from_filename(os.path.basename(file_path))\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            # Trích xuất tên\n",
    "            soup = BeautifulSoup(file, 'html.parser')\n",
    "            title = soup.title.string.strip() if soup.title else \"NOT FOUND\"\n",
    "            title = re.sub(r' - Cổng Light Novel - Đọc Light Novel$', '', title)\n",
    "            \n",
    "            # Trích xuất link\n",
    "            canonical_link = soup.find('link', rel='canonical')\n",
    "            canonical_url = canonical_link['href'] if canonical_link else \"NOT FOUND\"\n",
    "            canonical_url = canonical_url.replace('docln.net', 'ln.hako.vn')\n",
    "            \n",
    "            # Trích xuất phương thức dịch\n",
    "            method = \"NOT FOUND\"\n",
    "            method_span = soup.find('div', class_='series-type')\n",
    "            method_link = None\n",
    "            if method_span:\n",
    "                method_link = method_span.find('span')\n",
    "            if method_link and method_link.string:\n",
    "                method = method_link.string.strip()\n",
    "            \n",
    "            # Trích xuất thông tin về thể loại\n",
    "            genres = []\n",
    "            manga = \"Not sure\"\n",
    "            anime = \"Not sure\"\n",
    "            cd = \"Not sure\"\n",
    "            origin = \"japanese\"\n",
    "            genre_items = soup.find_all(class_='series-gerne-item')\n",
    "            for item in genre_items:\n",
    "                genre_text = item.get_text(strip=True)\n",
    "                if \"Manga\" in genre_text: manga = \"Yes\"\n",
    "                if \"Anime\" in genre_text: anime = \"Yes\"\n",
    "                if \"CD\" in genre_text: cd = \"Yes\"\n",
    "                if \"Chinese\" in genre_text: origin = \"chinese\"\n",
    "                if \"English\" in genre_text: origin = \"english\"\n",
    "                if \"Korean\" in genre_text: origin = \"korean\"\n",
    "                elif not any(word in genre_text for word in [\"Manga\", \"Anime\", \"CD\", \"Chinese\", \"English\", \"Korean\"]):\n",
    "                    genres.append(genre_text)\n",
    "            \n",
    "            # Trích xuất link ảnh\n",
    "            image_link = \"NOT FOUND\"\n",
    "            content_div = soup.find('div', class_='content img-in-ratio')\n",
    "            if content_div and 'style' in content_div.attrs:\n",
    "                style = content_div['style']\n",
    "                match = re.search(r\"url\\\\('([^']+)'\\\\)\", style)\n",
    "                if match:\n",
    "                    image_link = match.group(1)\n",
    "            \n",
    "            # Trích xuất tác giả\n",
    "            author = \"NOT FOUND\"\n",
    "            author_span = soup.find('span', class_='info-name', string='Tác giả:')\n",
    "            if author_span:\n",
    "                info_value_span = author_span.find_next_sibling('span', class_='info-value')\n",
    "                if info_value_span:\n",
    "                    author_link = info_value_span.find('a')\n",
    "                if author_link:\n",
    "                    author = author_link.string.strip()\n",
    "            \n",
    "            # Trích xuất họa sĩ\n",
    "            artist = \"NOT FOUND\"\n",
    "            artist_span = soup.find('span', class_='info-name', string='Họa sĩ:')\n",
    "            if artist_span:\n",
    "                info_value_span = artist_span.find_next_sibling('span', class_='info-value')\n",
    "                if info_value_span and info_value_span.string:\n",
    "                    artist = info_value_span.string.strip()\n",
    "            \n",
    "            # Trích xuất kiểu trình bày\n",
    "            showtype = 'light novel' if artist.lower() not in ['NOT FOUND', 'N/A'] else 'web novel'\n",
    "            \n",
    "            # Trích xuất tình trạng\n",
    "            state = \"NOT FOUND\"\n",
    "            state_span = soup.find('span', class_='info-name', string='Tình trạng:')\n",
    "            if state_span:\n",
    "                info_value_span = state_span.find_next_sibling('span', class_='info-value')\n",
    "                if info_value_span:\n",
    "                    state_link = info_value_span.find('a')\n",
    "                    if state_link:\n",
    "                        state = state_link.string.strip()\n",
    "            \n",
    "            # Trích xuất số like\n",
    "            like = 0\n",
    "            like_span = soup.find('span', class_='block feature-value')\n",
    "            if like_span:\n",
    "                like_link = like_span.find_next_sibling('span', class_='block feature-name')\n",
    "                if like_link and like_link.string:\n",
    "                    like_text = like_link.string.strip()\n",
    "                    try:\n",
    "                        like = float(like_text)\n",
    "                    except ValueError:\n",
    "                        pass\n",
    "            \n",
    "            # Trích xuất số từ\n",
    "            nword = 0\n",
    "            nword_span = soup.find('div', class_='statistic-name', string='Số từ')\n",
    "            if nword_span:\n",
    "                nword_link = nword_span.find_next_sibling('div', class_='statistic-value')\n",
    "                if nword_link:\n",
    "                    nword = nword_link.string.strip()\n",
    "            \n",
    "            # Trích xuất số lượt đánh giá\n",
    "            # rate = 0\n",
    "            # rate_span = soup.find('div', class_='statistic-name', string='Đánh giá')\n",
    "            # if rate_span:\n",
    "            #     rate_link = rate_span.find_next_sibling('div', class_='statistic-value')\n",
    "            #     if rate_link and rate_link.string:\n",
    "            #         rate_text = rate_link.string.strip()\n",
    "            #         try:\n",
    "            #             rate = float(rate_text)\n",
    "            #         except ValueError:\n",
    "            #             pass\n",
    "            \n",
    "            # Trích xuất số lượt xem\n",
    "            # view = 0\n",
    "            # view_span = soup.find('div', class_='statistic-name', string='Lượt xem')\n",
    "            # if view_span:\n",
    "            #     view_link = view_span.find_next_sibling('div', class_='statistic-value')\n",
    "            #     if view_link:\n",
    "            #         view = view_link.string.strip()\n",
    "            \n",
    "            # Trích xuất số lượt bình luận\n",
    "            # ncom = 0\n",
    "            # ncom_span = soup.find('span', class_='comments-count')\n",
    "            # if ncom_span:\n",
    "            #     ncom = ncom_span.string.strip()\n",
    "            #     ncom = re.sub(r'[()]', '', ncom)\n",
    "            \n",
    "            # Trích xuất tên gọi khác\n",
    "            fname = None\n",
    "            fname_span = soup.find('div', class_='fact-value')\n",
    "            if fname_span:\n",
    "                fname_links = fname_span.find_all('div', class_='block pad-bottom-5')\n",
    "                if fname_links:\n",
    "                    fname_list = [link.get_text(strip=True) for link in fname_links if link.get_text(strip=True)]\n",
    "                    fname = chr(10).join(fname_list) if fname_list else 'None'\n",
    "\n",
    "            # Trích xuất người dịch\n",
    "            trans = None\n",
    "            id_o_l = None\n",
    "            id_o = None\n",
    "            trans_span = soup.find('span', class_='series-owner_name')\n",
    "            if trans_span:\n",
    "                trans = trans_span.string.strip()\n",
    "                next_o = trans_span.find_next('a')\n",
    "                if next_o and 'href' in next_o.attrs:\n",
    "                    id_o_l = next_o['href']\n",
    "                    if not id_o_l.startswith(('https://', 'http://')):\n",
    "                        id_o_l = 'https://ln.hako.vn' + id_o_l\n",
    "                    else:\n",
    "                        id_o_l = id_o_l.replace('docln.net', 'ln.hako.vn')\n",
    "                        id_o = re.search(r'/(\\\\d+)$', id_o_l)\n",
    "                        id_o = id_o.group(1) if id_o else None\n",
    "\n",
    "            # Trích xuất nhóm dịch\n",
    "            team = None\n",
    "            id_t_l = None\n",
    "            id_t = None\n",
    "            team_span = soup.find('div', class_='fantrans-value')\n",
    "            if team_span:\n",
    "                team = team_span.string.strip()\n",
    "                next_t = team_span.find_next('a')\n",
    "                if next_t and 'href' in next_t.attrs:\n",
    "                    id_t_l = next_t['href']\n",
    "                    if not id_t_l.startswith(('https://', 'http://')):\n",
    "                        id_t_l = 'https://ln.hako.vn' + id_t_l\n",
    "                    else:\n",
    "                        id_t_l = id_t_l.replace('docln.net', 'ln.hako.vn')\n",
    "                    id_t = re.search(r'/nhom-dich/(\\\\d+)', id_t_l)\n",
    "                    id_t = id_t.group(1) if id_t else None\n",
    "\n",
    "            # Trích xuất người tham gia\n",
    "            atb = None\n",
    "            id_j_l = None\n",
    "            id_j = None\n",
    "            atb_span = soup.find('div', class_='series-owner_share')\n",
    "            if atb_span:\n",
    "                atb_links = atb_span.find_all('a', class_='ln_info-name')\n",
    "                if atb_links:\n",
    "                    atb_list = [link.get_text(strip=True) for link in atb_links if link.get_text(strip=True)]\n",
    "                    atb = chr(10).join(atb_list) if atb_list else 'None'\n",
    "                    id_j_l_list = [link.get('href') for link in atb_links if link.get('href')]\n",
    "                    id_j_l_list = ['https://ln.hako.vn' + url if not url.startswith(('https://', 'http://')) else url.replace('docln.net', 'ln.hako.vn') for url in id_j_l_list]\n",
    "                    id_j_l = chr(10).join(id_j_l_list) if id_j_l_list else 'None'\n",
    "                    id_j_list = [re.search(r'/(\\\\d+)$', url).group(1) for url in id_j_l_list if re.search(r'/(\\\\d+)$', url)]\n",
    "                    id_j = chr(10).join(id_j_list) if id_j_list else 'None'\n",
    "\n",
    "            # Trích xuất số tập\n",
    "            def count_vol(soup):\n",
    "                vol_spans = soup.find_all('span', class_='list_vol-title')\n",
    "                return len(vol_spans)\n",
    "            nvol = count_vol(soup)\n",
    "\n",
    "            # Trích xuất số chương\n",
    "            def count_chap(soup):\n",
    "                chap_spans = soup.find_all('div', class_='chapter-name')\n",
    "                return len(chap_spans)\n",
    "            nchap = count_chap(soup)\n",
    "\n",
    "            # Trích xuất thời gian bắt đầu và thời gian cập nhật mới nhất\n",
    "            first_day, first_month, first_year = None, None, None\n",
    "            latest_day, latest_month, latest_year = None, None, None\n",
    "            chapter_time_divs = soup.find_all('div', class_='chapter-time')\n",
    "            if chapter_time_divs:\n",
    "                first_date_text = chapter_time_divs[0].get_text(strip=True)\n",
    "                latest_date_text = chapter_time_divs[-1].get_text(strip=True)\n",
    "                first_day, first_month, first_year = parse_date(first_date_text)\n",
    "                latest_day, latest_month, latest_year = parse_date(latest_date_text)\n",
    "\n",
    "            return (file_id, title, canonical_url, method, genres, manga, anime, cd, origin, image_link,\n",
    "                    author, artist, showtype, state, like, nword,  fname, id_o, \n",
    "                    trans, id_o_l, id_t, team, id_t_l, id_j, atb, id_j_l, nvol, nchap, \n",
    "                    first_day, first_month, first_year, latest_day, latest_month, latest_year)\n",
    "            #Thêm rate, view, ncom nếu cần\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi trích xuất thông tin từ {file_path}: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã xử lý 10 file\n"
     ]
    }
   ],
   "source": [
    "# Hàm xử lý folder\n",
    "def process_folder(folder_path, max_files):\n",
    "    all_data = []\n",
    "    count = 0\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.html'):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            try:\n",
    "                # Trích xuất thông tin từ file HTML\n",
    "                info = extract_info_from_html(file_path)\n",
    "                # Kiểm tra xem info có phải là None không\n",
    "                if info is None:\n",
    "                    print(f\"Không thể trích xuất thông tin từ file: {filename}\")\n",
    "                    continue\n",
    "\n",
    "                # Giải nén thông tin\n",
    "                (file_id, title, canonical_url, method, genres, manga, anime, cd, origin, image_link,\n",
    "                 author, artist, showtype, state, like, nword, fname, id_o, \n",
    "                 trans, id_o_l, id_t, team, id_t_l, id_j, atb, id_j_l, nvol, nchap, \n",
    "                 first_day, first_month, first_year, latest_day, latest_month, latest_year) = info\n",
    "                \n",
    "                # Lưu thông tin vào DataFrame\n",
    "                all_data.append({\n",
    "                    'ID': process_value(file_id),\n",
    "                    'Tựa đề': process_value(title),\n",
    "                    'Link hako': process_value(canonical_url),\n",
    "                    'Phương thức dịch': process_value(method),\n",
    "                    'Thể loại': process_value(genres),\n",
    "                    'Manga': process_value(manga),\n",
    "                    'Anime': process_value(anime),\n",
    "                    'CD': process_value(cd),\n",
    "                    'Ngôn ngữ gốc': process_value(origin),\n",
    "                    'Link ảnh': process_value(image_link),\n",
    "                    'Tác giả': process_value(author),\n",
    "                    'Họa sĩ': process_value(artist),\n",
    "                    'Kiểu trình bày': process_value(showtype),\n",
    "                    'Tình trạng': process_value(state),\n",
    "                    'Số like': convert_to_number(process_value(like, is_numeric=True)),\n",
    "                    'Số từ': convert_to_number(process_value(nword, is_numeric=True)),\n",
    "                    # 'Số lượt đánh giá': convert_to_number(process_value(rate, is_numeric=True)),\n",
    "                    # 'Số lượt xem': convert_to_number(process_value(view, is_numeric=True)),\n",
    "                    # 'Số lượt bình luận': convert_to_number(process_value(ncom, is_numeric=True)),\n",
    "                    'Fname': process_value(fname),\n",
    "                    'ID người dịch': process_value(id_o),\n",
    "                    'Người dịch': process_value(trans),\n",
    "                    'Link người dịch': process_value(id_o_l),\n",
    "                    'ID nhóm dịch': process_value(id_t),\n",
    "                    'Nhóm dịch': process_value(team),\n",
    "                    'Link nhóm dịch': process_value(id_t_l),\n",
    "                    'ID người tham gia': process_value(id_j),\n",
    "                    'Người tham gia': process_value(atb),\n",
    "                    'Link người tham gia': process_value(id_j_l),\n",
    "                    'Số tập': process_value(nvol),\n",
    "                    'Số chương': process_value(nchap),\n",
    "                    'Ngày bắt đầu': process_value(first_day),\n",
    "                    'Tháng bắt đầu': process_value(first_month),   \n",
    "                    'Năm bắt đầu': process_value(first_year),\n",
    "                    'Ngày cập nhật cuối': process_value(latest_day),\n",
    "                    'Tháng cập nhật cuối': process_value(latest_month),\n",
    "                    'Năm cập nhật cuối': process_value(latest_year)\n",
    "                })\n",
    "\n",
    "                count += 1\n",
    "                if count >= max_files:\n",
    "                    break\n",
    "            except Exception as e:\n",
    "                print(f\"Lỗi khi xử lý file {filename}: {str(e)}\")\n",
    "\n",
    "    print(f\"Đã xử lý {count} file\")\n",
    "    return all_data\n",
    "\n",
    "max_files = 10 # Chỉnh số ở đây\n",
    "dataset = process_folder(folder_path, max_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm lưu thông tin vào file excel\n",
    "def save_to_excel(data, output_excel_path):\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_excel(output_excel_path, index=False, engine='openpyxl')\n",
    "    print(f\"Đã lưu thông tin vào file excel: {output_excel_path}\")\n",
    "\n",
    "# Hàm lưu thông tin vào file csv\n",
    "def save_to_csv(data, output_csv_path):\n",
    "    with open(output_csv_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        fieldnames = data[0].keys()\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for row in data:\n",
    "            writer.writerow(row)\n",
    "    print(f\"Đã lưu thông tin vào file csv: {output_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã xử lý 0 file\n"
     ]
    }
   ],
   "source": [
    "# Chỉnh số lượng file muốn trích xuất (trích toàn bộ thì max_files = total)\n",
    "max_files = 10 # Chỉnh số ở đây\n",
    "dataset = process_folder(folder_path, max_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã lưu thông tin vào file excel: hako_dataset.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Lưu vào file excel\n",
    "save_to_excel(dataset, output_excel_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã lưu thông tin vào file csv: hako_dataset.csv\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Luu vào file csv\n",
    "save_to_csv(dataset, output_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Muốn check gì thì check (nhớ sửa link)\n",
    "df_excel = pd.read_excel(r\"Hải Trần's folder\\extract + save data\\hako_dataset.xlsx\")\n",
    "df_csv = pd.read_csv(r\"Hải Trần's folder\\extract + save data\\hako_dataset.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
