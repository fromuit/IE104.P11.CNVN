{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup các kiểu\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import csv\n",
    "\n",
    "# Nhớ sửa mấy link đến folder và file \n",
    "folder_path = r'D:\\UIT\\Năm 3\\OLAP\\Dataset_backup\\hako-main\\docs\\truyen'\n",
    "output_excel_path = r'D:\\UIT\\Năm 3\\OLAP\\Dataset_backup\\hako-main\\hako_dataset_scrape.xlsx'\n",
    "output_csv_path = r'D:\\UIT\\Năm 3\\OLAP\\Dataset_backup\\hako-main\\hako_dataset_scrape.csv'\n",
    "\n",
    "# Nhớ chỉnh sửa max_files ở dưới trước khi trích xuất và lưu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tổng số file html trong folder là: 4938\n"
     ]
    }
   ],
   "source": [
    "# Các hàm xử lý phụ \n",
    "\n",
    "# Hàm xử lý ngày tháng năm\n",
    "def parse_date(date_text):\n",
    "    try:\n",
    "        date_obj = datetime.strptime(date_text, '%d/%m/%Y')\n",
    "        return date_obj.day, date_obj.month, date_obj.year\n",
    "    except ValueError:\n",
    "        print(f\"Unable to parse date: {date_text}\")\n",
    "        return None, None, None\n",
    "    \n",
    "# Hàm để xử lý giá trị rỗng\n",
    "def process_value(value, is_numeric=False):\n",
    "    if value in [None, 'nan', 'NaN', 'N/A', '']:\n",
    "        return 0 if is_numeric else \"NOT FOUND\"\n",
    "    return value    \n",
    "\n",
    "# Hàm chuyển đổi giá trị thành số\n",
    "def convert_to_number(value):\n",
    "    if isinstance(value, (int, float)):\n",
    "        return value\n",
    "    if isinstance(value, str):\n",
    "        value = ''.join(filter(str.isdigit, value))\n",
    "        return int(value) if value else 0\n",
    "    return 0\n",
    "\n",
    "# Hàm đếm số file html trong folder\n",
    "def count_html_files(folder_path):\n",
    "    html_count = 0\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.html'):\n",
    "            html_count += 1\n",
    "    return html_count\n",
    "\n",
    "total = count_html_files(folder_path)   # Đếm số file html trong folder\n",
    "print(f\"Tổng số file html trong folder là: {total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Các hàm trích xuất thông tin (muốn bỏ thông tin gì thì đóng comment đoạn code trích xuất thông tin đó)\n",
    "\n",
    "# Trích xuất ID từ tên file\n",
    "def extract_id_from_filename(filename):\n",
    "    match = re.match(r'^(\\d+)', filename)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "# Trích xuất thông tin từ file HTML\n",
    "def extract_info_from_html(file_path):\n",
    "    try:\n",
    "        file_id = extract_id_from_filename(os.path.basename(file_path))\n",
    "        \n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            # Trích xuất tên\n",
    "            soup = BeautifulSoup(file, 'html.parser')\n",
    "            title = soup.title.string.strip() if soup.title else \"NOT FOUND\"\n",
    "            title = re.sub(r' - Cổng Light Novel - Đọc Light Novel$', '', title) \n",
    "            \n",
    "            # Trích xuất link\n",
    "            canonical_link = soup.find('link', rel='canonical')\n",
    "            canonical_url = canonical_link['href'] if canonical_link else \"NOT FOUND\"\n",
    "            canonical_url = canonical_url.replace('docln.net', 'ln.hako.vn')\n",
    "\n",
    "            # Trích xuất phương thức dịch\n",
    "            method = \"NOT FOUND\"\n",
    "            method_span = soup.find('div', class_='series-type')\n",
    "            if method_span:\n",
    "                method_link = method_span.find('span')\n",
    "            if method_link and method_link.string:\n",
    "                method = method_link.string.strip()\n",
    "\n",
    "            # Trích xuất thông tin về thể loại\n",
    "            genres = []\n",
    "            manga = \"Not sure\"\n",
    "            anime = \"Not sure\"\n",
    "            cd = \"Not sure\"\n",
    "            origin = \"japanese\"\n",
    "            genre_items = soup.find_all(class_='series-gerne-item')\n",
    "            for item in genre_items:\n",
    "                genre_text = item.get_text(strip=True)\n",
    "                if \"Manga\" in genre_text: manga = \"Yes\"\n",
    "                if \"Anime\" in genre_text: anime = \"Yes\"\n",
    "                if \"CD\" in genre_text: cd = \"Yes\"\n",
    "                if \"Chinese\" in genre_text: origin = \"chinese\"\n",
    "                if \"English\" in genre_text: origin = \"english\"\n",
    "                if \"Korean\" in genre_text: origin = \"korean\"\n",
    "                elif not any(word in genre_text for word in [\"Manga\", \"Anime\", \"CD\", \"Chinese\", \"English\", \"Korean\"]):\n",
    "                    genres.append(genre_text)\n",
    "\n",
    "            # Trích xuất link ảnh\n",
    "            image_link = \"NOT FOUND\"\n",
    "            content_div = soup.find('div', class_='content img-in-ratio')\n",
    "            if content_div and 'style' in content_div.attrs:\n",
    "                style = content_div['style']\n",
    "                match = re.search(r\"url\\('([^']+)'\\)\", style)\n",
    "                if match:\n",
    "                    image_link = match.group(1)\n",
    "            \n",
    "            # Trích xuất tác giả\n",
    "            author = \"NOT FOUND\"\n",
    "            author_span = soup.find('span', class_='info-name', string='Tác giả:')\n",
    "            if author_span:\n",
    "                info_value_span = author_span.find_next_sibling('span', class_='info-value')\n",
    "                if info_value_span:\n",
    "                    author_link = info_value_span.find('a')\n",
    "                if author_link:\n",
    "                    author = author_link.string.strip()\n",
    "        \n",
    "            # Trích xuất họa sĩ\n",
    "            artist = \"NOT FOUND\"\n",
    "            artist_span = soup.find('span', class_='info-name', string='Họa sĩ:')\n",
    "            if artist_span:\n",
    "                info_value_span = artist_span.find_next_sibling('span', class_='info-value')\n",
    "                if info_value_span and info_value_span.string:\n",
    "                    artist = info_value_span.string.strip()\n",
    "\n",
    "            # Trích xuất kiểu trình bày\n",
    "            showtype = 'light novel' if artist.lower() not in ['NOT FOUND', 'N/A'] else 'web novel'  \n",
    "\n",
    "            # Trích xuất tình trạng\n",
    "            state = \"NOT FOUND\"\n",
    "            state_span = soup.find('span', class_='info-name', string='Tình trạng:')\n",
    "            if state_span:\n",
    "                info_value_span = state_span.find_next_sibling('span', class_='info-value')\n",
    "                if info_value_span:\n",
    "                    state_link = info_value_span.find('a')\n",
    "                    if state_link:\n",
    "                        state = state_link.string.strip() \n",
    "    \n",
    "            # Trích xuất số like\n",
    "            like = 0\n",
    "            like_span = soup.find('span', class_='block feature-value')\n",
    "            if like_span:\n",
    "                like_link = like_span.find_next_sibling('span', class_='block feature-name')\n",
    "                if like_link and like_link.string:\n",
    "                    like_text = like_link.string.strip()\n",
    "                    try:\n",
    "                        like = float(like_text)\n",
    "                    except ValueError:\n",
    "                        pass\n",
    "\n",
    "            # Trích xuất số từ\n",
    "            nword = 0\n",
    "            nword_span = soup.find('div', class_='statistic-name', string='Số từ')\n",
    "            if nword_span:\n",
    "                nword_link = nword_span.find_next_sibling('div', class_='statistic-value')\n",
    "                if nword_link:\n",
    "                    nword = nword_link.string.strip()\n",
    "\n",
    "            # Trích xuất số lượt đánh giá\n",
    "            rate = 0  \n",
    "            rate_span = soup.find('div', class_='statistic-name', string='Đánh giá')\n",
    "            if rate_span:\n",
    "                rate_link = rate_span.find_next_sibling('div', class_='statistic-value')\n",
    "                if rate_link and rate_link.string:\n",
    "                    rate_text = rate_link.string.strip()\n",
    "                    try:\n",
    "                        rate = float(rate_text)\n",
    "                    except ValueError:\n",
    "                        pass\n",
    "\n",
    "            # Trích xuất số lượt xem\n",
    "            view = 0\n",
    "            view_span = soup.find('div', class_='statistic-name', string='Lượt xem')\n",
    "            if view_span:\n",
    "                view_link = view_span.find_next_sibling('div', class_='statistic-value')\n",
    "                if view_link:\n",
    "                    view = view_link.string.strip()\n",
    "\n",
    "            # Trích xuất số lượt bình luận\n",
    "            ncom = 0\n",
    "            ncom_span = soup.find('span', class_='comments-count')\n",
    "            if ncom_span:\n",
    "                ncom = ncom_span.string.strip()\n",
    "                ncom = re.sub(r'[()]', '', ncom)\n",
    "\n",
    "            # Trích xuất tên gọi khác\n",
    "            fname = None\n",
    "            fname_span = soup.find('div', class_='fact-value')\n",
    "            if fname_span:\n",
    "                fname_links = fname_span.find_all('div', class_='block pad-bottom-5')\n",
    "                if fname_links:\n",
    "                    fname_list = [link.get_text(strip=True) for link in fname_links if link.get_text(strip=True)]\n",
    "                    fname = chr(10).join(fname_list) if fname_list else 'None'\n",
    "\n",
    "            # Trích xuất người dịch\n",
    "            trans = None\n",
    "            id_o_l = None\n",
    "            id_o = None\n",
    "            trans_span = soup.find('span', class_='series-owner_name')\n",
    "            if trans_span:\n",
    "                trans = trans_span.string.strip()\n",
    "                next_o = trans_span.find_next('a')\n",
    "                if next_o and 'href' in next_o.attrs:\n",
    "                    id_o_l = next_o['href']\n",
    "                    if not id_o_l.startswith(('https://', 'http://')):\n",
    "                        id_o_l = 'https://ln.hako.vn' + id_o_l\n",
    "                    else:\n",
    "                        id_o_l = id_o_l.replace('docln.net', 'ln.hako.vn')\n",
    "                        id_o = re.search(r'/(\\d+)$', id_o_l)\n",
    "                        id_o = id_o.group(1) if id_o else None\n",
    "\n",
    "            # Trích xuất nhóm dịch\n",
    "            team = None\n",
    "            id_t_l = None\n",
    "            id_t = None\n",
    "            team_span = soup.find('div', class_='fantrans-value')\n",
    "            if team_span:\n",
    "                team = team_span.string.strip()\n",
    "                next_t = team_span.find_next('a')\n",
    "                if next_t and 'href' in next_t.attrs:\n",
    "                    id_t_l = next_t['href']\n",
    "                    if not id_t_l.startswith(('https://', 'http://')):\n",
    "                        id_t_l = 'https://ln.hako.vn' + id_t_l\n",
    "                    else:\n",
    "                        id_t_l = id_t_l.replace('docln.net', 'ln.hako.vn')\n",
    "                    id_t = re.search(r'/nhom-dich/(\\d+)', id_t_l)\n",
    "                    id_t = id_t.group(1) if id_t else None\n",
    "\n",
    "            # Trích xuất người tham gia\n",
    "            atb = None\n",
    "            id_j_l = None\n",
    "            id_j = None  \n",
    "            atb_span = soup.find('div', class_='series-owner_share')\n",
    "            if atb_span:\n",
    "                atb_links = atb_span.find_all('a', class_='ln_info-name')\n",
    "                if atb_links:\n",
    "                    atb_list = [link.get_text(strip=True) for link in atb_links if link.get_text(strip=True)]\n",
    "                    atb = chr(10).join(atb_list) if atb_list else 'None'\n",
    "                    id_j_l_list = [link.get('href') for link in atb_links if link.get('href')]\n",
    "                    id_j_l_list = ['https://ln.hako.vn' + url if not url.startswith(('https://', 'http://')) else url.replace('docln.net', 'ln.hako.vn') for url in id_j_l_list]\n",
    "                    id_j_l = chr(10).join(id_j_l_list) if id_j_l_list else 'None'\n",
    "                    id_j_list = [re.search(r'/(\\d+)$', url).group(1) for url in id_j_l_list if re.search(r'/(\\d+)$', url)]\n",
    "                    id_j = chr(10).join(id_j_list) if id_j_list else 'None'\n",
    "\n",
    "            # Trích xuất số tập\n",
    "            def count_vol(soup):\n",
    "                vol_spans = soup.find_all('span', class_='list_vol-title')\n",
    "                return len(vol_spans)\n",
    "            nvol = count_vol(soup)\n",
    "\n",
    "            # Trích xuất số chương\n",
    "            def count_chap(soup):\n",
    "                chap_spans = soup.find_all('div', class_='chapter-name')\n",
    "                return len(chap_spans)\n",
    "            nchap = count_chap(soup)\n",
    "\n",
    "            # Trích xuất thời gian bắt đầu và thời gian cập nhật mới nhất\n",
    "            first_day, first_month, first_year = None, None, None\n",
    "            latest_day, latest_month, latest_year = None, None, None\n",
    "\n",
    "            chapter_time_divs = soup.find_all('div', class_='chapter-time')\n",
    "            if chapter_time_divs:\n",
    "                first_date_text = chapter_time_divs[0].get_text(strip=True)\n",
    "                latest_date_text = chapter_time_divs[-1].get_text(strip=True)\n",
    "            \n",
    "                first_day, first_month, first_year = parse_date(first_date_text)\n",
    "                latest_day, latest_month, latest_year = parse_date(latest_date_text)\n",
    "\n",
    "        return (file_id, title, canonical_url, method, genres, manga, anime, cd, origin, image_link,\n",
    "                author, artist, showtype, state, like, nword, rate, view, fname, id_o, \n",
    "                trans, id_o_l, id_t, team, id_t_l, id_j, atb, id_j_l, nvol, nchap, \n",
    "                first_day, first_month, first_year, latest_day, latest_month, latest_year, ncom)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi trích xuất thông tin từ {file_path}: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm xử lý folder\n",
    "def process_folder(folder_path, max_files):\n",
    "    all_data = []\n",
    "    count = 0\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.html'):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            try:\n",
    "                # Trích xuất thông tin từ file HTML\n",
    "                info = extract_info_from_html(file_path)\n",
    "                # Kiểm tra xem info có phải là None không\n",
    "                if info is None:\n",
    "                    print(f\"Không thể trích xuất thông tin từ file: {filename}\")\n",
    "                    continue\n",
    "\n",
    "                # Giải nén thông tin\n",
    "                (file_id, title, canonical_url, method, genres, manga, anime, cd, origin, image_link,\n",
    "                 author, artist, showtype, state, like, nword, rate, view, fname, id_o, \n",
    "                 trans, id_o_l, id_t, team, id_t_l, id_j, atb, id_j_l, nvol, nchap, \n",
    "                 first_day, first_month, first_year, latest_day, latest_month, latest_year, ncom) = info\n",
    "                \n",
    "                # Lưu thông tin vào DataFrame\n",
    "                all_data.append({\n",
    "                    'ID': process_value(file_id),\n",
    "                    'Tựa đề': process_value(title),\n",
    "                    'Link hako': process_value(canonical_url),\n",
    "                    'Phương thức dịch': process_value(method),\n",
    "                    'Thể loại': process_value(genres),\n",
    "                    'Manga': process_value(manga),\n",
    "                    'Anime': process_value(anime),\n",
    "                    'CD': process_value(cd),\n",
    "                    'Ngôn ngữ gốc': process_value(origin),\n",
    "                    'Link ảnh': process_value(image_link),\n",
    "                    'Tác giả': process_value(author),\n",
    "                    'Họa sĩ': process_value(artist),\n",
    "                    'Kiểu trình bày': process_value(showtype),\n",
    "                    'Tình trạng': process_value(state),\n",
    "                    'Số like': convert_to_number(process_value(like, is_numeric=True)),\n",
    "                    'Số từ': convert_to_number(process_value(nword, is_numeric=True)),\n",
    "                    'Số lượt đánh giá': convert_to_number(process_value(rate, is_numeric=True)),\n",
    "                    'Số lượt xem': convert_to_number(process_value(view, is_numeric=True)),\n",
    "                    'Số lượt bình luận': convert_to_number(process_value(ncom, is_numeric=True)),\n",
    "                    'Fname': process_value(fname),\n",
    "                    'ID người dịch': process_value(id_o),\n",
    "                    'Người dịch': process_value(trans),\n",
    "                    'Link người dịch': process_value(id_o_l),\n",
    "                    'ID nhóm dịch': process_value(id_t),\n",
    "                    'Nhóm dịch': process_value(team),\n",
    "                    'Link nhóm dịch': process_value(id_t_l),\n",
    "                    'ID người tham gia': process_value(id_j),\n",
    "                    'Người tham gia': process_value(atb),\n",
    "                    'Link người tham gia': process_value(id_j_l),\n",
    "                    'Số tập': process_value(nvol),\n",
    "                    'Số chương': process_value(nchap),\n",
    "                    'Ngày bắt đầu': process_value(first_day),\n",
    "                    'Tháng bắt đầu': process_value(first_month),   \n",
    "                    'Năm bắt đầu': process_value(first_year),\n",
    "                    'Ngày cập nhật cuối': process_value(latest_day),\n",
    "                    'Tháng cập nhật cuối': process_value(latest_month),\n",
    "                    'Năm cập nhật cuối': process_value(latest_year)\n",
    "                })\n",
    "\n",
    "                count += 1\n",
    "                if count >= max_files:\n",
    "                    break\n",
    "            except Exception as e:\n",
    "                print(f\"Lỗi khi xử lý file {filename}: {str(e)}\")\n",
    "\n",
    "    print(f\"Đã xử lý {count} file\")\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm lưu thông tin vào file excel\n",
    "def save_to_excel(data, output_excel_path):\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_excel(output_excel_path, index=False, engine='openpyxl')\n",
    "    print(f\"Đã lưu thông tin vào file excel: {output_excel_path}\")\n",
    "\n",
    "# Hàm lưu thông tin vào file csv\n",
    "def save_to_csv(data, output_csv_path):\n",
    "    with open(output_csv_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        fieldnames = data[0].keys()\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for row in data:\n",
    "            writer.writerow(row)\n",
    "    print(f\"Đã lưu thông tin vào file csv: {output_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã xử lý 10 file\n"
     ]
    }
   ],
   "source": [
    "# Chỉnh số lượng file muốn trích xuất (trích toàn bộ thì max_files = total)\n",
    "max_files = 10 # Chỉnh số ở đây\n",
    "dataset = process_folder(folder_path, max_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã lưu thông tin vào file excel: D:\\UIT\\Năm 3\\OLAP\\Dataset_backup\\hako-main\\hako_dataset_scrape.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Lưu vào file excel\n",
    "save_to_excel(dataset, output_excel_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã lưu thông tin vào file csv: D:\\UIT\\Năm 3\\OLAP\\Dataset_backup\\hako-main\\hako_dataset_scrape.csv\n"
     ]
    }
   ],
   "source": [
    "# Luu vào file csv\n",
    "save_to_csv(dataset, output_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Tựa đề</th>\n",
       "      <th>Link hako</th>\n",
       "      <th>Phương thức dịch</th>\n",
       "      <th>Thể loại</th>\n",
       "      <th>Manga</th>\n",
       "      <th>Anime</th>\n",
       "      <th>CD</th>\n",
       "      <th>Ngôn ngữ gốc</th>\n",
       "      <th>Link ảnh</th>\n",
       "      <th>...</th>\n",
       "      <th>Người tham gia</th>\n",
       "      <th>Link người tham gia</th>\n",
       "      <th>Số tập</th>\n",
       "      <th>Số chương</th>\n",
       "      <th>Ngày bắt đầu</th>\n",
       "      <th>Tháng bắt đầu</th>\n",
       "      <th>Năm bắt đầu</th>\n",
       "      <th>Ngày cập nhật cuối</th>\n",
       "      <th>Tháng cập nhật cuối</th>\n",
       "      <th>Năm cập nhật cuối</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Seirei Tsukai no Blade Dance</td>\n",
       "      <td>https://ln.hako.vn/truyen/1-seirei-tsukai-no-b...</td>\n",
       "      <td>Truyện dịch</td>\n",
       "      <td>['Action', 'Adventure', 'Comedy', 'Fantasy', '...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>japanese</td>\n",
       "      <td>https://i.docln.net/lightnovel/covers/s1-c4dd8...</td>\n",
       "      <td>...</td>\n",
       "      <td>NOT FOUND</td>\n",
       "      <td>NOT FOUND</td>\n",
       "      <td>21</td>\n",
       "      <td>272</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>2016</td>\n",
       "      <td>20</td>\n",
       "      <td>7</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>Utsuro no Hako to Zero no Maria</td>\n",
       "      <td>https://ln.hako.vn/truyen/10-utsuro-no-hako-to...</td>\n",
       "      <td>Truyện dịch</td>\n",
       "      <td>['Drama', 'Mystery', 'Romance']</td>\n",
       "      <td>Not sure</td>\n",
       "      <td>Not sure</td>\n",
       "      <td>Not sure</td>\n",
       "      <td>japanese</td>\n",
       "      <td>https://3.bp.blogspot.com/-hGKnPrypenQ/WO2vaWF...</td>\n",
       "      <td>...</td>\n",
       "      <td>NOT FOUND</td>\n",
       "      <td>NOT FOUND</td>\n",
       "      <td>6</td>\n",
       "      <td>25</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>2016</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>Accel World</td>\n",
       "      <td>https://ln.hako.vn/truyen/100-accel-world</td>\n",
       "      <td>Truyện dịch</td>\n",
       "      <td>['Action', 'Harem', 'School Life', 'Science Fi...</td>\n",
       "      <td>Not sure</td>\n",
       "      <td>Not sure</td>\n",
       "      <td>Not sure</td>\n",
       "      <td>japanese</td>\n",
       "      <td>https://4.bp.blogspot.com/--os7wMK9P4s/WO2voFt...</td>\n",
       "      <td>...</td>\n",
       "      <td>NOT FOUND</td>\n",
       "      <td>NOT FOUND</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>2016</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10001</td>\n",
       "      <td>Watashi ga Koibito ni Nareru Wake Najian Muri ...</td>\n",
       "      <td>https://ln.hako.vn/truyen/10001-watashi-ga-koi...</td>\n",
       "      <td>Truyện dịch</td>\n",
       "      <td>['Harem', 'Romance', 'School Life', 'Yuri']</td>\n",
       "      <td>Not sure</td>\n",
       "      <td>Not sure</td>\n",
       "      <td>Not sure</td>\n",
       "      <td>japanese</td>\n",
       "      <td>https://c1.hako.re/lightnovel/covers/s10001-a0...</td>\n",
       "      <td>...</td>\n",
       "      <td>NOT FOUND</td>\n",
       "      <td>NOT FOUND</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>2021</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10002</td>\n",
       "      <td>Wortenia senki - Cổng Light Novel</td>\n",
       "      <td>https://ln.hako.vn/truyen/10002-wortenia-senki</td>\n",
       "      <td>Truyện dịch</td>\n",
       "      <td>['Action', 'Adventure', 'Harem', 'Isekai', 'Sh...</td>\n",
       "      <td>Not sure</td>\n",
       "      <td>Not sure</td>\n",
       "      <td>Not sure</td>\n",
       "      <td>japanese</td>\n",
       "      <td>https://c1.hako.re/lightnovel/covers/s10002-39...</td>\n",
       "      <td>...</td>\n",
       "      <td>NOT FOUND</td>\n",
       "      <td>NOT FOUND</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>2021</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID                                             Tựa đề  \\\n",
       "0      1                       Seirei Tsukai no Blade Dance   \n",
       "1     10                    Utsuro no Hako to Zero no Maria   \n",
       "2    100                                        Accel World   \n",
       "3  10001  Watashi ga Koibito ni Nareru Wake Najian Muri ...   \n",
       "4  10002                  Wortenia senki - Cổng Light Novel   \n",
       "\n",
       "                                           Link hako Phương thức dịch  \\\n",
       "0  https://ln.hako.vn/truyen/1-seirei-tsukai-no-b...      Truyện dịch   \n",
       "1  https://ln.hako.vn/truyen/10-utsuro-no-hako-to...      Truyện dịch   \n",
       "2          https://ln.hako.vn/truyen/100-accel-world      Truyện dịch   \n",
       "3  https://ln.hako.vn/truyen/10001-watashi-ga-koi...      Truyện dịch   \n",
       "4     https://ln.hako.vn/truyen/10002-wortenia-senki      Truyện dịch   \n",
       "\n",
       "                                            Thể loại     Manga     Anime  \\\n",
       "0  ['Action', 'Adventure', 'Comedy', 'Fantasy', '...       Yes       Yes   \n",
       "1                    ['Drama', 'Mystery', 'Romance']  Not sure  Not sure   \n",
       "2  ['Action', 'Harem', 'School Life', 'Science Fi...  Not sure  Not sure   \n",
       "3        ['Harem', 'Romance', 'School Life', 'Yuri']  Not sure  Not sure   \n",
       "4  ['Action', 'Adventure', 'Harem', 'Isekai', 'Sh...  Not sure  Not sure   \n",
       "\n",
       "         CD Ngôn ngữ gốc                                           Link ảnh  \\\n",
       "0       Yes     japanese  https://i.docln.net/lightnovel/covers/s1-c4dd8...   \n",
       "1  Not sure     japanese  https://3.bp.blogspot.com/-hGKnPrypenQ/WO2vaWF...   \n",
       "2  Not sure     japanese  https://4.bp.blogspot.com/--os7wMK9P4s/WO2voFt...   \n",
       "3  Not sure     japanese  https://c1.hako.re/lightnovel/covers/s10001-a0...   \n",
       "4  Not sure     japanese  https://c1.hako.re/lightnovel/covers/s10002-39...   \n",
       "\n",
       "   ... Người tham gia Link người tham gia Số tập Số chương  Ngày bắt đầu  \\\n",
       "0  ...      NOT FOUND           NOT FOUND     21       272            11   \n",
       "1  ...      NOT FOUND           NOT FOUND      6        25            11   \n",
       "2  ...      NOT FOUND           NOT FOUND      2        14            11   \n",
       "3  ...      NOT FOUND           NOT FOUND      1         1            13   \n",
       "4  ...      NOT FOUND           NOT FOUND      1         2            13   \n",
       "\n",
       "   Tháng bắt đầu  Năm bắt đầu  Ngày cập nhật cuối  Tháng cập nhật cuối  \\\n",
       "0             11         2016                  20                    7   \n",
       "1             11         2016                  11                   11   \n",
       "2             11         2016                  11                   11   \n",
       "3              8         2021                  13                    8   \n",
       "4              8         2021                  13                    8   \n",
       "\n",
       "  Năm cập nhật cuối  \n",
       "0              2020  \n",
       "1              2016  \n",
       "2              2016  \n",
       "3              2021  \n",
       "4              2021  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Muốn check gì thì check (nhớ sửa link)\n",
    "df_excel = pd.read_excel(r'D:\\UIT\\Năm 3\\OLAP\\Dataset_backup\\hako-main\\hako_dataset_scrape.xlsx')\n",
    "df_csv = pd.read_csv(r'D:\\UIT\\Năm 3\\OLAP\\Dataset_backup\\hako-main\\hako_dataset_scrape.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
