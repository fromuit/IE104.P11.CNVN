{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup các kiểu\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import csv\n",
    "import json\n",
    "\n",
    "# Nhớ sửa mấy link đến folder và file \n",
    "folder_path = r'E:\\Web\\IE104.P11.CNVN\\truyen'\n",
    "output_excel_path = r'hako_data.xlsx'\n",
    "output_csv_path = r'hako_data.csv'\n",
    "output_json_path = r'hako_data.json'\n",
    "\n",
    "# Nhớ chỉnh sửa max_files ở dưới trước khi trích xuất và lưu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tổng số file html trong folder là: 176\n"
     ]
    }
   ],
   "source": [
    "# Các hàm xử lý phụ \n",
    "\n",
    "# Hàm xử lý ngày tháng năm\n",
    "def parse_date(date_text):\n",
    "    try:\n",
    "        date_obj = datetime.strptime(date_text, '%d/%m/%Y')\n",
    "        return date_obj.day, date_obj.month, date_obj.year\n",
    "    except ValueError:\n",
    "        print(f\"Unable to parse date: {date_text}\")\n",
    "        return None, None, None\n",
    "    \n",
    "# Hàm để xử lý giá trị rỗng\n",
    "def process_value(value, is_numeric=False):\n",
    "    if value in [None, 'nan', 'NaN', 'N/A', '']:\n",
    "        return 0 if is_numeric else \"NOT FOUND\"\n",
    "    return value    \n",
    "\n",
    "# Hàm chuyển đổi giá trị thành số\n",
    "def convert_to_number(value):\n",
    "    if isinstance(value, (int, float)):\n",
    "        return value\n",
    "    if isinstance(value, str):\n",
    "        value = ''.join(filter(str.isdigit, value))\n",
    "        return int(value) if value else 0\n",
    "    return 0\n",
    "\n",
    "# Hàm đếm số file html trong folder\n",
    "def count_html_files(folder_path):\n",
    "    html_count = 0\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.html'):\n",
    "            html_count += 1\n",
    "    return html_count\n",
    "\n",
    "total = count_html_files(folder_path)   # Đếm số file html trong folder\n",
    "print(f\"Tổng số file html trong folder là: {total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Các hàm trích xuất thông tin (muốn bỏ thông tin gì thì đóng comment đoạn code trích xuất thông tin đó)\n",
    "\n",
    "# Trích xuất ID từ tên file\n",
    "def extract_id_from_filename(filename):\n",
    "    match = re.match(r'^(\\d+)', filename)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "# Trích xuất thông tin từ file HTML\n",
    "def extract_info_from_html(file_path):\n",
    "    try:\n",
    "        file_id = extract_id_from_filename(os.path.basename(file_path))\n",
    "        \n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            # Trích xuất tên\n",
    "            soup = BeautifulSoup(file, 'html.parser')\n",
    "            title = soup.title.string.strip() if soup.title else \"NOT FOUND\"\n",
    "            title = re.sub(r' - Cổng Light Novel - Đọc Light Novel$', '', title) \n",
    "            \n",
    "            # Trích xuất link\n",
    "            canonical_link = soup.find('link', rel='canonical')\n",
    "            canonical_url = canonical_link['href'] if canonical_link else \"NOT FOUND\"\n",
    "            canonical_url = canonical_url.replace('docln.net', 'ln.hako.vn')\n",
    "\n",
    "            # Trích xuất phương thức dịch\n",
    "            method = \"NOT FOUND\"\n",
    "            method_span = soup.find('div', class_='series-type')\n",
    "            if method_span:\n",
    "                method_link = method_span.find('span')\n",
    "            if method_link and method_link.string:\n",
    "                method = method_link.string.strip()\n",
    "\n",
    "            # Trích xuất thông tin về thể loại\n",
    "            genres = []\n",
    "            manga = \"Not sure\"\n",
    "            anime = \"Not sure\"\n",
    "            cd = \"Not sure\"\n",
    "            origin = \"japanese\"\n",
    "            genre_items = soup.find_all(class_='series-gerne-item')\n",
    "            for item in genre_items:\n",
    "                genre_text = item.get_text(strip=True)\n",
    "                if \"Manga\" in genre_text: manga = \"Yes\"\n",
    "                elif \"Anime\" in genre_text: anime = \"Yes\"\n",
    "                elif \"CD\" in genre_text: cd = \"Yes\"\n",
    "                elif \"Chinese\" in genre_text: origin = \"chinese\"\n",
    "                elif \"English\" in genre_text: origin = \"english\"\n",
    "                elif \"Korean\" in genre_text: origin = \"korean\"\n",
    "                elif not any(word in genre_text for word in [\"Manga\", \"Anime\", \"CD\", \"Chinese\", \"English\", \"Korean\"]):\n",
    "                    genres.append(genre_text)\n",
    "\n",
    "            # Trích xuất link ảnh\n",
    "            image_link = \"NOT FOUND\"\n",
    "            content_div = soup.find('div', class_='content img-in-ratio')\n",
    "            if content_div and 'style' in content_div.attrs:\n",
    "                style = content_div['style']\n",
    "                match = re.search(r\"url\\('([^']+)'\\)\", style)\n",
    "                if match:\n",
    "                    image_link = match.group(1)\n",
    "            \n",
    "            # Trích xuất tác giả\n",
    "            author = \"NOT FOUND\"\n",
    "            author_span = soup.find('span', class_='info-name', string='Tác giả:')\n",
    "            if author_span:\n",
    "                info_value_span = author_span.find_next_sibling('span', class_='info-value')\n",
    "                if info_value_span:\n",
    "                    author_link = info_value_span.find('a')\n",
    "                if author_link:\n",
    "                    author = author_link.string.strip()\n",
    "        \n",
    "            # Trích xuất họa sĩ\n",
    "            artist = \"NOT FOUND\"\n",
    "            artist_span = soup.find('span', class_='info-name', string='Họa sĩ:')\n",
    "            if artist_span:\n",
    "                info_value_span = artist_span.find_next_sibling('span', class_='info-value')\n",
    "                if info_value_span and info_value_span.string:\n",
    "                    artist = info_value_span.string.strip()\n",
    "\n",
    "            # Trích xuất kiểu trình bày\n",
    "            showtype = 'light novel' if artist != \"NOT FOUND\" else 'web novel'  \n",
    "\n",
    "            # Trích xuất tình trạng\n",
    "            state = \"NOT FOUND\"\n",
    "            state_span = soup.find('span', class_='info-name', string='Tình trạng:')\n",
    "            if state_span:\n",
    "                info_value_span = state_span.find_next_sibling('span', class_='info-value')\n",
    "                if info_value_span:\n",
    "                    state_link = info_value_span.find('a')\n",
    "                    if state_link:\n",
    "                        state = state_link.string.strip() \n",
    "    \n",
    "            # Trích xuất số like\n",
    "            like = 0\n",
    "            like_span = soup.find('span', class_='block feature-value')\n",
    "            if like_span:\n",
    "                like_link = like_span.find_next_sibling('span', class_='block feature-name')\n",
    "                if like_link and like_link.string:\n",
    "                    like_text = like_link.string.strip()\n",
    "                    try:\n",
    "                        like = float(like_text)\n",
    "                    except ValueError:\n",
    "                        pass\n",
    "\n",
    "            # Trích xuất số từ\n",
    "            nword = 0\n",
    "            nword_span = soup.find('div', class_='statistic-name', string='Số từ')\n",
    "            if nword_span:\n",
    "                nword_link = nword_span.find_next_sibling('div', class_='statistic-value')\n",
    "                if nword_link:\n",
    "                    nword = nword_link.string.strip()\n",
    "\n",
    "            # Trích xuất số lượt đánh giá\n",
    "            rate = 0  \n",
    "            rate_span = soup.find('div', class_='statistic-name', string='Đánh giá')\n",
    "            if rate_span:\n",
    "                rate_link = rate_span.find_next_sibling('div', class_='statistic-value')\n",
    "                if rate_link and rate_link.string:\n",
    "                    rate_text = rate_link.string.strip()\n",
    "                    try:\n",
    "                        rate = float(rate_text)\n",
    "                    except ValueError:\n",
    "                        pass\n",
    "\n",
    "            # Trích xuất số lượt xem\n",
    "            view = 0\n",
    "            view_span = soup.find('div', class_='statistic-name', string='Lượt xem')\n",
    "            if view_span:\n",
    "                view_link = view_span.find_next_sibling('div', class_='statistic-value')\n",
    "                if view_link:\n",
    "                    view = view_link.string.strip()\n",
    "\n",
    "            # Trích xuất số lượt bình luận\n",
    "            ncom = 0\n",
    "            ncom_span = soup.find('span', class_='comments-count')\n",
    "            if ncom_span:\n",
    "                ncom = ncom_span.string.strip()\n",
    "                ncom = re.sub(r'[()]', '', ncom)\n",
    "\n",
    "            # Trích xuất tên gọi khác\n",
    "            fname = None\n",
    "            fname_span = soup.find('div', class_='fact-value')\n",
    "            if fname_span:\n",
    "                fname_links = fname_span.find_all('div', class_='block pad-bottom-5')\n",
    "                if fname_links:\n",
    "                    fname_list = [link.get_text(strip=True) for link in fname_links if link.get_text(strip=True)]\n",
    "                    fname = chr(10).join(fname_list) if fname_list else 'None'\n",
    "\n",
    "            # Trích xuất người dịch\n",
    "            trans = None\n",
    "            id_o_l = None\n",
    "            id_o = None\n",
    "            trans_span = soup.find('span', class_='series-owner_name')\n",
    "            if trans_span:\n",
    "                trans = trans_span.string.strip()\n",
    "                next_o = trans_span.find_next('a')\n",
    "                if next_o and 'href' in next_o.attrs:\n",
    "                    id_o_l = next_o['href']\n",
    "                    if not id_o_l.startswith(('https://', 'http://')):\n",
    "                        id_o_l = 'https://ln.hako.vn' + id_o_l\n",
    "                    else:\n",
    "                        id_o_l = id_o_l.replace('docln.net', 'ln.hako.vn')\n",
    "                        id_o = re.search(r'/(\\d+)$', id_o_l)\n",
    "                        id_o = id_o.group(1) if id_o else None\n",
    "\n",
    "            # Trích xuất nhóm dịch\n",
    "            team = None\n",
    "            id_t_l = None\n",
    "            id_t = None\n",
    "            team_span = soup.find('div', class_='fantrans-value')\n",
    "            if team_span:\n",
    "                team = team_span.string.strip()\n",
    "                next_t = team_span.find_next('a')\n",
    "                if next_t and 'href' in next_t.attrs:\n",
    "                    id_t_l = next_t['href']\n",
    "                    if not id_t_l.startswith(('https://', 'http://')):\n",
    "                        id_t_l = 'https://ln.hako.vn' + id_t_l\n",
    "                    else:\n",
    "                        id_t_l = id_t_l.replace('docln.net', 'ln.hako.vn')\n",
    "                    id_t = re.search(r'/nhom-dich/(\\d+)', id_t_l)\n",
    "                    id_t = id_t.group(1) if id_t else None\n",
    "\n",
    "            # Trích xuất người tham gia\n",
    "            atb = None\n",
    "            id_j_l = None\n",
    "            id_j = None  \n",
    "            atb_span = soup.find('div', class_='series-owner_share')\n",
    "            if atb_span:\n",
    "                atb_links = atb_span.find_all('a', class_='ln_info-name')\n",
    "                if atb_links:\n",
    "                    atb_list = [link.get_text(strip=True) for link in atb_links if link.get_text(strip=True)]\n",
    "                    atb = chr(10).join(atb_list) if atb_list else 'None'\n",
    "                    id_j_l_list = [link.get('href') for link in atb_links if link.get('href')]\n",
    "                    id_j_l_list = ['https://ln.hako.vn' + url if not url.startswith(('https://', 'http://')) else url.replace('docln.net', 'ln.hako.vn') for url in id_j_l_list]\n",
    "                    id_j_l = chr(10).join(id_j_l_list) if id_j_l_list else 'None'\n",
    "                    id_j_list = [re.search(r'/(\\d+)$', url).group(1) for url in id_j_l_list if re.search(r'/(\\d+)$', url)]\n",
    "                    id_j = chr(10).join(id_j_list) if id_j_list else 'None'\n",
    "\n",
    "            # Trích xuất số tập\n",
    "            def count_vol(soup):\n",
    "                vol_spans = soup.find_all('span', class_='list_vol-title')\n",
    "                return len(vol_spans)\n",
    "            nvol = count_vol(soup)\n",
    "\n",
    "            # Trích xuất số chương\n",
    "            def count_chap(soup):\n",
    "                chap_spans = soup.find_all('div', class_='chapter-name')\n",
    "                return len(chap_spans)\n",
    "            nchap = count_chap(soup)\n",
    "\n",
    "            # Trích xuất thời gian bắt đầu và thời gian cập nhật mới nhất\n",
    "            first_day, first_month, first_year = None, None, None\n",
    "            latest_day, latest_month, latest_year = None, None, None\n",
    "\n",
    "            chapter_time_divs = soup.find_all('div', class_='chapter-time')\n",
    "            if chapter_time_divs:\n",
    "                first_date_text = chapter_time_divs[0].get_text(strip=True)\n",
    "                latest_date_text = chapter_time_divs[-1].get_text(strip=True)\n",
    "            \n",
    "                first_day, first_month, first_year = parse_date(first_date_text)\n",
    "                latest_day, latest_month, latest_year = parse_date(latest_date_text)\n",
    "\n",
    "        return (file_id, title, canonical_url, method, genres, manga, anime, cd, origin, image_link,\n",
    "                author, artist, showtype, state, like, nword, rate, view, fname, id_o, \n",
    "                trans, id_o_l, id_t, team, id_t_l, id_j, atb, id_j_l, nvol, nchap, \n",
    "                first_day, first_month, first_year, latest_day, latest_month, latest_year, ncom)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi trích xuất thông tin từ {file_path}: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm lọc file HTML trùng với tên thư mục con\n",
    "def filter_matching_html_files(folder_path):\n",
    "    # Lấy danh sách tất cả thư mục con\n",
    "    subdirs = [d for d in os.listdir(folder_path) \n",
    "              if os.path.isdir(os.path.join(folder_path, d))]\n",
    "    \n",
    "    # Lấy danh sách file html\n",
    "    html_files = [f for f in os.listdir(folder_path) \n",
    "                 if f.endswith('.html')]\n",
    "    \n",
    "    # Lọc các file html có tên trùng với thư mục con\n",
    "    matching_files = []\n",
    "    for html_file in html_files:\n",
    "        # Lấy tên file không có đuôi .html\n",
    "        file_name = os.path.splitext(html_file)[0]\n",
    "        if file_name in subdirs:\n",
    "            matching_files.append(html_file)\n",
    "    \n",
    "    print(f\"Tìm thấy {len(matching_files)} file HTML trùng với tên thư mục con\")\n",
    "    return matching_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm xử lý folder\n",
    "def process_folder(folder_path, start_index=0, end_index=None):\n",
    "    all_data = []\n",
    "    count = 0\n",
    "    total_processed = 0\n",
    "\n",
    "    # # Lấy danh sách tất cả file html và sắp xếp theo tên\n",
    "    # html_files = sorted([f for f in os.listdir(folder_path) if f.endswith('.html')])\n",
    "    html_files = filter_matching_html_files(folder_path)\n",
    "    \n",
    "    # Xác định vị trí bắt đầu và kết thúc\n",
    "    start = max(0, start_index)\n",
    "    end = min(len(html_files), end_index) if end_index else len(html_files)\n",
    "    \n",
    "    # Chỉ xử lý các file trong khoảng đã chọn\n",
    "    for filename in html_files[start:end]:\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        try:\n",
    "            # Phần code xử lý file giữ nguyên như cũ\n",
    "            info = extract_info_from_html(file_path)\n",
    "            if info is None:\n",
    "                print(f\"Không thể trích xuất thông tin từ file: {filename}\")\n",
    "                continue\n",
    "\n",
    "                # Giải nén thông tin\n",
    "            (file_id, title, canonical_url, method, genres, manga, anime, cd, origin, image_link,\n",
    "                 author, artist, showtype, state, like, nword, rate, view, fname, id_o, \n",
    "                 trans, id_o_l, id_t, team, id_t_l, id_j, atb, id_j_l, nvol, nchap, \n",
    "                first_day, first_month, first_year, latest_day, latest_month, latest_year, ncom) = info\n",
    "                \n",
    "            # Lưu thông tin vào DataFrame\n",
    "            all_data.append({\n",
    "                'ID': process_value(file_id),\n",
    "                'Tựa đề': process_value(title),\n",
    "                'Link truyện': process_value(canonical_url),\n",
    "                'Phương thức dịch': process_value(method),\n",
    "                'Thể loại': process_value(genres),\n",
    "                'Manga': process_value(manga),\n",
    "                'Anime': process_value(anime),\n",
    "                'CD': process_value(cd),\n",
    "                'Ngôn ngữ gốc': process_value(origin),\n",
    "                'Link ảnh': process_value(image_link),\n",
    "                'Tác giả': process_value(author),\n",
    "                'Họa sĩ': process_value(artist),\n",
    "                'Kiểu trình bày': process_value(showtype),\n",
    "                'Tình trạng': process_value(state),\n",
    "                'Số like': convert_to_number(process_value(like, is_numeric=True)),\n",
    "                'Số từ': convert_to_number(process_value(nword, is_numeric=True)),\n",
    "                'Số lượt đánh giá': convert_to_number(process_value(rate, is_numeric=True)),\n",
    "                'Số lượt xem': convert_to_number(process_value(view, is_numeric=True)),\n",
    "                'Số lượt bình luận': convert_to_number(process_value(ncom, is_numeric=True)),\n",
    "                'Fname': process_value(fname),\n",
    "                'ID người dịch': process_value(id_o),\n",
    "                'Người dịch': process_value(trans),\n",
    "                'Link người dịch': process_value(id_o_l),\n",
    "                'ID nhóm dịch': process_value(id_t),\n",
    "                'Nhóm dịch': process_value(team),\n",
    "                'Link nhóm dịch': process_value(id_t_l),\n",
    "                'ID người tham gia': process_value(id_j),\n",
    "                'Người tham gia': process_value(atb),\n",
    "                'Link người tham gia': process_value(id_j_l),\n",
    "                'Số tập': process_value(nvol),\n",
    "                'Số chương': process_value(nchap),\n",
    "                'Ngày bắt đầu': process_value(first_day),\n",
    "                'Tháng bắt đầu': process_value(first_month),   \n",
    "                'Năm bắt đầu': process_value(first_year),\n",
    "                'Ngày cập nhật cuối': process_value(latest_day),\n",
    "                'Tháng cập nhật cuối': process_value(latest_month),\n",
    "                'Năm cập nhật cuối': process_value(latest_year)\n",
    "            })\n",
    "\n",
    "            count += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Lỗi khi xử lý file {filename}: {str(e)}\")\n",
    "\n",
    "    print(f\"Đã xử lý {count} file\")\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm lưu thông tin vào file excel\n",
    "def save_to_excel(data, output_excel_path):\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_excel(output_excel_path, index=False, engine='openpyxl')\n",
    "    print(f\"Đã lưu thông tin vào file excel: {output_excel_path}\")\n",
    "\n",
    "# Hàm lưu thông tin vào file csv\n",
    "def save_to_csv(data, output_csv_path):\n",
    "    with open(output_csv_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        fieldnames = data[0].keys()\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for row in data:\n",
    "            writer.writerow(row)\n",
    "    print(f\"Đã lưu thông tin vào file csv: {output_csv_path}\")\n",
    "\n",
    "def save_to_json(data, output_json_path):\n",
    "    with open(output_json_path, 'w', encoding='utf-8') as jsonfile:\n",
    "        json.dump(data, jsonfile, ensure_ascii=False, indent=4)\n",
    "    print(f\"Đã lưu thông tin vào file json: {output_json_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tìm thấy 29 file HTML trùng với tên thư mục con\n",
      "Đã xử lý 29 file\n"
     ]
    }
   ],
   "source": [
    "# Chỉnh số lượng file muốn trích xuất (trích toàn bộ thì max_files = total)\n",
    "dataset = process_folder(folder_path, start_index=0, end_index=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã lưu thông tin vào file excel: hako_data.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Lưu vào file excel\n",
    "save_to_excel(dataset, output_excel_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã lưu thông tin vào file csv: hako_data.csv\n"
     ]
    }
   ],
   "source": [
    "# Luu vào file csv\n",
    "save_to_csv(dataset, output_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã lưu thông tin vào file json: hako_data.json\n"
     ]
    }
   ],
   "source": [
    "# Luu vào file json\n",
    "save_to_json(dataset, output_json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'D:\\\\test_react\\\\web_wibu_react\\\\extract\\\\hako_data.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Muốn check gì thì check (nhớ sửa link)\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df_excel \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mD:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mtest_react\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mweb_wibu_react\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mextract\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mhako_data.xlsx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m df_csv \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mtest_react\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mweb_wibu_react\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mextract\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mhako_data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mtest_react\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mweb_wibu_react\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mextract\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mhako_data.json\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[1;32mc:\\Users\\NguyễnHoàngDuy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:495\u001b[0m, in \u001b[0;36mread_excel\u001b[1;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[0m\n\u001b[0;32m    493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(io, ExcelFile):\n\u001b[0;32m    494\u001b[0m     should_close \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 495\u001b[0m     io \u001b[38;5;241m=\u001b[39m \u001b[43mExcelFile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;129;01mand\u001b[39;00m engine \u001b[38;5;241m!=\u001b[39m io\u001b[38;5;241m.\u001b[39mengine:\n\u001b[0;32m    502\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    503\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEngine should not be specified when passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    504\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    505\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\NguyễnHoàngDuy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:1550\u001b[0m, in \u001b[0;36mExcelFile.__init__\u001b[1;34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001b[0m\n\u001b[0;32m   1548\u001b[0m     ext \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxls\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1549\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1550\u001b[0m     ext \u001b[38;5;241m=\u001b[39m \u001b[43minspect_excel_format\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1551\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\n\u001b[0;32m   1552\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1553\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ext \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1554\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1555\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExcel file format cannot be determined, you must specify \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1556\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man engine manually.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1557\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\NguyễnHoàngDuy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:1402\u001b[0m, in \u001b[0;36minspect_excel_format\u001b[1;34m(content_or_path, storage_options)\u001b[0m\n\u001b[0;32m   1399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(content_or_path, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[0;32m   1400\u001b[0m     content_or_path \u001b[38;5;241m=\u001b[39m BytesIO(content_or_path)\n\u001b[1;32m-> 1402\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1403\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m   1404\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handle:\n\u001b[0;32m   1405\u001b[0m     stream \u001b[38;5;241m=\u001b[39m handle\u001b[38;5;241m.\u001b[39mhandle\n\u001b[0;32m   1406\u001b[0m     stream\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\NguyễnHoàngDuy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\common.py:882\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m--> 882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    883\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[0;32m    885\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'D:\\\\test_react\\\\web_wibu_react\\\\extract\\\\hako_data.xlsx'"
     ]
    }
   ],
   "source": [
    "# Muốn check gì thì check (nhớ sửa link)\n",
    "df_excel = pd.read_excel(r'D:\\test_react\\web_wibu_react\\extract\\hako_data.xlsx')\n",
    "df_csv = pd.read_csv(r'D:\\test_react\\web_wibu_react\\extract\\hako_data.csv')\n",
    "with open(r'D:\\test_react\\web_wibu_react\\extract\\hako_data.json', 'r', encoding='utf-8') as f:\n",
    "    df_json = json.load(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
